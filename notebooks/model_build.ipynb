{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "random_seed = 46\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "#dropping all of the observations that are very likely errors\n",
    "dataset = pd.read_csv(\"data/master_dataset.csv\")\n",
    "dataset = dataset[dataset[\"cases_hrs\"] <= 300]\n",
    "dataset = dataset[dataset[\"Total_Hours\"] >= 10]\n",
    "\n",
    "\n",
    "#dropping all of the uneeded columns\n",
    "to_drop = [\"Date\", \"Total_Hours\", \"Total_Cases\", \"B_HrsPct\", \"B_Cases\", \"Total_Each_Day\", \"dry_ratio\", \"clr_ratio\", \"frz_ratio\", \"GO_LIVE_DATE\", \"LABEL_TYPE\"]\n",
    "dataset_build = dataset.drop(labels=to_drop, axis=1)\n",
    "dataset_build.rename(columns={\"BRNCH_CD\":\"brnch_cd\", \"A_HrsPct\":\"a_hrspct\", \"C_HrsPct\":\"c_hrspct\", \"A_Cases\":\"a_cases\", \"C_Cases\":\"c_cases\"}, inplace=True)\n",
    "dataset_build.to_csv(\"data/model_dataset.csv\",index=False)\n",
    "\n",
    "numeric_features = [\"a_hrspct\",  'c_hrspct', 'a_cases', 'c_cases']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_features=['brnch_cd', 'weekday', 'month']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_build.drop(labels=\"cases_hrs\", axis=1),dataset_build['cases_hrs'], random_state=random_seed, train_size = .70)\n",
    "\n",
    "scores = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3352.2183            4.71m\n",
      "         2        3131.8694            4.72m\n",
      "         3        2976.2394            4.71m\n",
      "         4        2866.8875            4.65m\n",
      "         5        2776.4135            4.59m\n",
      "         6        2704.9141            4.50m\n",
      "         7        2652.6289            4.37m\n",
      "         8        2617.3720            4.21m\n",
      "         9        2568.4399            4.13m\n",
      "        10        2522.4693            4.08m\n",
      "        20        2179.4723            3.73m\n",
      "        30        1940.9241            3.55m\n",
      "        40        1742.9652            3.42m\n",
      "        50        1592.6744            3.35m\n",
      "        60        1483.7383            3.27m\n",
      "        70        1387.9131            3.20m\n",
      "        80        1299.9089            3.15m\n",
      "        90        1210.3370            3.11m\n",
      "       100        1118.3258            3.08m\n",
      "       200         653.0818            2.68m\n",
      "       300         465.2375            2.31m\n",
      "       400         364.9373            1.97m\n",
      "       500         323.7699            1.63m\n",
      "       600         298.7779            1.30m\n",
      "       700         284.6439           58.09s\n",
      "       800         275.7749           38.64s\n",
      "       900         271.6691           19.28s\n",
      "      1000         269.3532            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3523.6235            1.55m\n",
      "         2        3408.4655            1.54m\n",
      "         3        3325.1834            1.55m\n",
      "         4        3268.6674            1.54m\n",
      "         5        3230.4894            1.54m\n",
      "         6        3195.8809            1.55m\n",
      "         7        3175.0434            1.53m\n",
      "         8        3146.5474            1.52m\n",
      "         9        3132.2447            1.50m\n",
      "        10        3116.7177            1.49m\n",
      "        20        2987.6423            1.43m\n",
      "        30        2903.1577            1.40m\n",
      "        40        2835.6883            1.37m\n",
      "        50        2781.7571            1.35m\n",
      "        60        2723.5126            1.33m\n",
      "        70        2670.3651            1.32m\n",
      "        80        2623.0443            1.30m\n",
      "        90        2575.6307            1.29m\n",
      "       100        2539.2232            1.27m\n",
      "       200        2219.1642            1.14m\n",
      "       300        1975.1852            1.00m\n",
      "       400        1785.7270           51.65s\n",
      "       500        1628.8282           42.97s\n",
      "       600        1484.3641           34.40s\n",
      "       700        1370.4846           25.82s\n",
      "       800        1269.0962           17.23s\n",
      "       900        1169.8350            8.64s\n",
      "      1000        1092.3926            0.00s\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "reg_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", LinearRegression())]\n",
    ")\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Reg\", reg_pipe.score(X_test, y_test)])\n",
    "\n",
    "#gradient boosting regressor model\n",
    "gbr_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(random_state=random_seed))]\n",
    ")\n",
    "gbr_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Init\", gbr_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr\n",
    "gbr_improved_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=10, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr 2.0 - lower depth\n",
    "gbr_improved_pipe_2 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=5, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_2.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe_2.score(X_test, y_test)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GBR Grid Search\n",
    "* Takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngbrgrid = {\\'n_estimators\\':[500,1000,2000], \\n           \\'learning_rate\\':[0.15,0.3], \\n           \\'max_depth\\':[5,10]}\\n\\ngbrgrid_pipe = Pipeline(\\n    steps=[(\"preprocessor\", preprocessor), (\"gbrsearch\", GridSearchCV(estimator = GradientBoostingRegressor(), \\n                                                                      param_grid = gbrgrid,\\n                                                                      scoring = \\'neg_mean_squared_error\\',\\n                                                                      verbose = 1,\\n                                                                      cv = crossvalidation))]\\n)\\ngbrsearch = gbrgrid_pipe.fit(X_train, y_train)\\n#gbrgrid_pipe.best_params_, gbrgrid_pipe.best_score_\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrgrid = {'n_estimators':[500,1000,2000], \n",
    "           'learning_rate':[0.15,0.3], \n",
    "           'max_depth':[5,10]}\n",
    "\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "\n",
    "gbrgrid_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"gbrsearch\", GridSearchCV(estimator = GradientBoostingRegressor(), \n",
    "                                                                      param_grid = gbrgrid,\n",
    "                                                                      scoring = 'neg_mean_squared_error',\n",
    "                                                                      verbose = 1,\n",
    "                                                                      cv = crossvalidation))]\n",
    ")\n",
    "gbrsearch = gbrgrid_pipe.fit(X_train, y_train)\n",
    "gbrgrid_pipe.best_params_, gbrgrid_pipe.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3672.8376            4.09m\n",
      "         2        3634.7779            4.05m\n",
      "         3        3600.4718            4.03m\n",
      "         4        3566.9995            4.04m\n",
      "         5        3536.5007            4.04m\n",
      "         6        3508.7512            4.06m\n",
      "         7        3484.6493            4.07m\n",
      "         8        3459.9013            4.04m\n",
      "         9        3438.6136            4.01m\n",
      "        10        3415.3945            4.01m\n",
      "        20        3259.2275            3.98m\n",
      "        30        3164.2132            3.92m\n",
      "        40        3104.2060            3.84m\n",
      "        50        3065.1080            3.73m\n",
      "        60        3025.0354            3.67m\n",
      "        70        2993.7219            3.62m\n",
      "        80        2964.4815            3.59m\n",
      "        90        2938.8806            3.56m\n",
      "       100        2916.1085            3.53m\n",
      "       200        2733.5650            3.27m\n",
      "       300        2600.1975            3.06m\n",
      "       400        2485.2323            2.84m\n",
      "       500        2387.4672            2.64m\n",
      "       600        2292.3107            2.46m\n",
      "       700        2210.0009            2.27m\n",
      "       800        2136.5710            2.09m\n",
      "       900        2071.3199            1.92m\n",
      "      1000        2004.5392            1.74m\n",
      "      2000        1538.2919            0.00s\n"
     ]
    }
   ],
   "source": [
    "#improved gbr\n",
    "    ## to reduce variablity, depth for gbr should be between 4 - 8\n",
    "    ## n_estimators = 2000\n",
    "    ## smaller learning rate = 0.05 - 0.15 is good\n",
    "    ## 4 minutes\n",
    "gbr_improved_pipe_3 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, \n",
    "                                                                               n_estimators=2000, \n",
    "                                                                               learning_rate=0.05,\n",
    "                                                                               max_depth=6,  \n",
    "                                                                               loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_3.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved 3\",gbr_improved_pipe_3.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees Regressor\n",
    "* Best MSE / RMSE = 242.242 / 15.564\n",
    "    * With DEFAULT paramters, performs pretty well against tuned GBR\n",
    "    * increasing n_estimators does not change much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.2s\n"
     ]
    }
   ],
   "source": [
    "#Extra trees base model, 5 1/2 minutes\n",
    "xtratree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", ExtraTreesRegressor(verbose=1))]\n",
    ")\n",
    "xtratree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"xtratree\",xtratree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "* Best MSE / RMSE = 261.028 / 16.156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncrossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\\nrandfor_param = {\\n             'max_depth': [5, 10, 15],\\n             'n_estimators': [500, 1000, 1500]}\\nrandfor_search = GridSearchCV(RandomForestRegressor(), randfor_param, refit = True, verbose = 3, cv = crossvalidation, scoring = 'neg_mean_squared_error')\\nrandfor_search.fit(X_train, y_train)\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#random forest grid search\n",
    "## run if needed\n",
    "\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "randfor_param = {\n",
    "             'max_depth': [5, 10, 15],\n",
    "             'n_estimators': [500, 1000, 1500]}\n",
    "randfor_search = GridSearchCV(RandomForestRegressor(), randfor_param, refit = True, verbose = 3, cv = crossvalidation, scoring = 'neg_mean_squared_error')\n",
    "randfor_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.2s\n"
     ]
    }
   ],
   "source": [
    "#baseline RFR, outperformed by extratrees regressor - 4 min\n",
    "randfor_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(verbose=1))]\n",
    ")\n",
    "randfor_pipe.fit(X_train, y_train)\n",
    "scores.append([\"RFR\", randfor_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor\n",
    "* Best MSE / RMSE = 466.094 / 21.589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline Decision tree regressor\n",
    "dectree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", DecisionTreeRegressor())]\n",
    ")\n",
    "dectree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Decision Tree\",dectree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GBR Improved 3', 0.11209127690946474],\n",
       " ['xtratree', 0.03662133309109494],\n",
       " ['RFR', 0.08079112462153293],\n",
       " ['Decision Tree', -0.6565450546367695]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/xtra_tree.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pickling the model \n",
    "from joblib import dump\n",
    "\n",
    "dump(reg_pipe, \"models/lin_reg.joblib\") \n",
    "dump(gbr_pipe, \"models/gbr_init.joblib\")\n",
    "dump(gbr_improved_pipe, \"models/gbr_improved.joblib\")\n",
    "dump(gbr_improved_pipe_2, \"models/gbr_improved_2.joblib\")\n",
    "dump(gbr_improved_pipe_3, \"models/gbr_improved_3.joblib\")\n",
    "dump(randfor_pipe, \"models/rfr.joblib\")\n",
    "dump(dectree_pipe, \"models/dec_tree.joblib\")\n",
    "dump(xtratree_pipe, \"models/xtra_tree.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
