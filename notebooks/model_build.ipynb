{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "random_seed = 46\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "jobs = -1 #this is the number of cores that the models and test will run on. -1 means that all cores will be used \n",
    "\n",
    "#dropping all of the observations that are very likely errors\n",
    "dataset = pd.read_csv(\"data/master_dataset.csv\")\n",
    "dataset = dataset[dataset[\"cases_hrs\"] <= 300]\n",
    "dataset = dataset[dataset[\"Total_Hours\"] >= 10]\n",
    "\n",
    "\n",
    "#dropping all of the uneeded columns\n",
    "to_drop = [\"Date\", \"Total_Hours\", \"Total_Cases\", \"B_HrsPct\", \"B_Cases\", \"Total_Each_Day\", \"dry_ratio\", \"clr_ratio\", \"frz_ratio\", \"GO_LIVE_DATE\", \"LABEL_TYPE\"]\n",
    "dataset_build = dataset.drop(labels=to_drop, axis=1)\n",
    "dataset_build.rename(columns={\"BRNCH_CD\":\"brnch_cd\", \"A_HrsPct\":\"a_hrspct\", \"C_HrsPct\":\"c_hrspct\", \"A_Cases\":\"a_cases\", \"C_Cases\":\"c_cases\"}, inplace=True)\n",
    "dataset_build.to_csv(\"data/model_dataset.csv\",index=False)\n",
    "\n",
    "numeric_features = [\"a_hrspct\",  'c_hrspct', 'a_cases', 'c_cases']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "categorical_features=['brnch_cd', 'weekday', 'month']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_build.drop(labels=\"cases_hrs\", axis=1),dataset_build['cases_hrs'], random_state=random_seed, train_size = .70)\n",
    "\n",
    "scores = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3352.2183            4.79m\n",
      "         2        3131.8694            4.78m\n",
      "         3        2976.2394            4.88m\n",
      "         4        2866.8875            4.79m\n",
      "         5        2776.4135            4.68m\n",
      "         6        2704.9141            4.56m\n",
      "         7        2652.6289            4.40m\n",
      "         8        2617.3720            4.22m\n",
      "         9        2568.4399            4.14m\n",
      "        10        2522.4693            4.07m\n",
      "        20        2179.4723            3.69m\n",
      "        30        1940.9241            3.50m\n",
      "        40        1742.9652            3.36m\n",
      "        50        1592.6744            3.28m\n",
      "        60        1483.7383            3.20m\n",
      "        70        1387.9131            3.14m\n",
      "        80        1299.9089            3.08m\n",
      "        90        1210.3370            3.04m\n",
      "       100        1118.3258            3.00m\n",
      "       200         653.0818            2.60m\n",
      "       300         465.2375            2.24m\n",
      "       400         364.9373            1.91m\n",
      "       500         323.7699            1.58m\n",
      "       600         298.7779            1.26m\n",
      "       700         284.6439           56.62s\n",
      "       800         275.7749           37.68s\n",
      "       900         271.6691           18.78s\n",
      "      1000         269.3532            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3523.6235            1.51m\n",
      "         2        3408.4655            1.50m\n",
      "         3        3325.1834            1.51m\n",
      "         4        3268.6674            1.51m\n",
      "         5        3230.4894            1.50m\n",
      "         6        3195.8809            1.51m\n",
      "         7        3175.0434            1.49m\n",
      "         8        3146.5474            1.50m\n",
      "         9        3132.2447            1.48m\n",
      "        10        3116.7177            1.47m\n",
      "        20        2987.6423            1.43m\n",
      "        30        2903.1577            1.40m\n",
      "        40        2835.6883            1.38m\n",
      "        50        2781.7571            1.35m\n",
      "        60        2723.5126            1.34m\n",
      "        70        2670.3651            1.32m\n",
      "        80        2623.0443            1.31m\n",
      "        90        2575.6307            1.30m\n",
      "       100        2539.2232            1.28m\n",
      "       200        2219.1642            1.15m\n",
      "       300        1975.1852            1.01m\n",
      "       400        1785.7270           51.86s\n",
      "       500        1628.8282           43.12s\n",
      "       600        1484.3641           34.54s\n",
      "       700        1370.4846           25.93s\n",
      "       800        1269.0962           17.29s\n",
      "       900        1169.8350            8.68s\n",
      "      1000        1092.3926            0.00s\n"
     ]
    }
   ],
   "source": [
    "#linear regression\n",
    "reg_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", LinearRegression())]\n",
    ")\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Reg\", reg_pipe.score(X_test, y_test)])\n",
    "\n",
    "#gradient boosting regressor model\n",
    "gbr_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(random_state=random_seed))]\n",
    ")\n",
    "gbr_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Init\", gbr_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr\n",
    "gbr_improved_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=10, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe.score(X_test, y_test)])\n",
    "\n",
    "#improved gbr 2.0 - lower depth\n",
    "gbr_improved_pipe_2 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, n_estimators=1000, learning_rate=0.3, max_depth=5, random_state=random_seed, loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_2.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved\", gbr_improved_pipe_2.score(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GBR Grid Search\n",
    "* Takes a while to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gbrgrid = {'n_estimators':[500,1000,2000], \n",
    "           'learning_rate':[0.15,0.3], \n",
    "           'max_depth':[5,10]}\n",
    "\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "\n",
    "gbrgrid_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"gbrsearch\", GridSearchCV(estimator = GradientBoostingRegressor(), \n",
    "                                                                      param_grid = gbrgrid,\n",
    "                                                                      scoring = 'neg_mean_squared_error',\n",
    "                                                                      verbose = 1,\n",
    "                                                                      cv = crossvalidation, n_jobs=jobs))]\n",
    ")\n",
    "gbrsearch = gbrgrid_pipe.fit(X_train, y_train)\n",
    "gbrgrid_pipe.best_params_, gbrgrid_pipe.best_score_\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1        3672.8376            6.79m\n",
      "         2        3634.7779            6.64m\n",
      "         3        3600.4718            6.71m\n",
      "         4        3566.9995            6.62m\n",
      "         5        3536.5007            6.57m\n",
      "         6        3508.7512            6.56m\n",
      "         7        3484.6493            6.55m\n",
      "         8        3459.9013            6.51m\n",
      "         9        3438.6136            6.52m\n",
      "        10        3415.3945            6.55m\n",
      "        20        3259.2275            6.51m\n",
      "        30        3164.2132            6.43m\n",
      "        40        3104.2060            6.35m\n",
      "        50        3065.1080            6.19m\n",
      "        60        3025.0354            5.78m\n",
      "        70        2993.7219            5.41m\n",
      "        80        2964.4815            5.14m\n",
      "        90        2938.8806            4.92m\n",
      "       100        2916.1085            4.73m\n",
      "       200        2733.5650            3.78m\n",
      "       300        2600.1975            3.34m\n",
      "       400        2485.2323            3.04m\n",
      "       500        2387.4672            2.79m\n",
      "       600        2292.3107            2.57m\n",
      "       700        2210.0009            2.37m\n",
      "       800        2136.5710            2.17m\n",
      "       900        2071.3199            1.98m\n",
      "      1000        2004.5392            1.79m\n",
      "      2000        1538.2919            0.00s\n"
     ]
    }
   ],
   "source": [
    "#improved gbr\n",
    "    ## to reduce variablity, depth for gbr should be between 4 - 8\n",
    "    ## n_estimators = 2000\n",
    "    ## smaller learning rate = 0.05 - 0.15 is good\n",
    "    ## 4 minutes\n",
    "gbr_improved_pipe_3 = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", GradientBoostingRegressor(verbose=1, \n",
    "                                                                               n_estimators=2000, \n",
    "                                                                               learning_rate=0.05,\n",
    "                                                                               max_depth=6,  \n",
    "                                                                               loss='squared_error'))]\n",
    ")\n",
    "gbr_improved_pipe_3.fit(X_train, y_train)\n",
    "scores.append([\"GBR Improved 3\",gbr_improved_pipe_3.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra Trees Regressor\n",
    "* Best MSE / RMSE = 242.242 / 15.564\n",
    "    * With DEFAULT paramters, performs pretty well against tuned GBR\n",
    "    * increasing n_estimators does not change much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#Extra trees base model, 5 1/2 minutes\n",
    "xtratree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", ExtraTreesRegressor(verbose=1, n_jobs = jobs))]\n",
    ")\n",
    "xtratree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"xtratree\",xtratree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor\n",
    "* Best MSE / RMSE = 261.028 / 16.156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest grid search\n",
    "## run if needed\n",
    "\"\"\"\n",
    "crossvalidation=KFold(n_splits=3,shuffle=True,random_state=1)\n",
    "randfor_param = {\n",
    "             'max_depth': [5, 10, 15],\n",
    "             'n_estimators': [500, 1000, 1500]}\n",
    "randfor_search = GridSearchCV(RandomForestRegressor(n_jobs=jobs), randfor_param, refit = True, verbose = 3, cv = crossvalidation, scoring = 'neg_mean_squared_error', n_jobs=jobs)\n",
    "randfor_search.fit(X_train, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   46.6s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#baseline RFR, outperformed by extratrees regressor - 4 min\n",
    "randfor_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", RandomForestRegressor(verbose=1, n_jobs=jobs))]\n",
    ")\n",
    "randfor_pipe.fit(X_train, y_train)\n",
    "scores.append([\"RFR\", randfor_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor\n",
    "* Best MSE / RMSE = 466.094 / 21.589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#baseline Decision tree regressor\n",
    "dectree_pipe = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"model\", DecisionTreeRegressor())]\n",
    ")\n",
    "dectree_pipe.fit(X_train, y_train)\n",
    "scores.append([\"Decision Tree\", dectree_pipe.score(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Reg', 0.050882787387249384],\n",
       " ['GBR Init', 0.1376223526075423],\n",
       " ['GBR Improved', -0.08274349859880914],\n",
       " ['GBR Improved', 0.04058212754023327],\n",
       " ['GBR Improved 3', 0.11209127690946474],\n",
       " ['xtratree', 0.03662133309109494],\n",
       " ['RFR', 0.08079112462153282],\n",
       " ['Decision Tree', -0.6565450546367695]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/xtra_tree.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END .......max_depth=5, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/3] END ......max_depth=5, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 2/3] END ......max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END .....max_depth=10, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 3/3] END .....max_depth=10, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 3/3] END .....max_depth=10, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 2/3] END .....max_depth=15, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 2/3] END ......max_depth=5, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 3/3] END ......max_depth=15, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/3] END .....max_depth=15, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 3/3] END ......max_depth=5, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 2/3] END ......max_depth=15, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END .....max_depth=15, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END ......max_depth=5, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 2/3] END .....max_depth=15, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END .......max_depth=5, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END ......max_depth=5, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 2/3] END ......max_depth=5, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END ......max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/3] END ......max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/3] END .....max_depth=10, n_estimators=1000;, score=nan total time=   0.0s\n",
      "[CV 1/3] END .....max_depth=10, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END ......max_depth=15, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/3] END .....max_depth=15, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 3/3] END .......max_depth=5, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/3] END .....max_depth=10, n_estimators=1500;, score=nan total time=   0.0s\n",
      "[CV 1/3] END .....max_depth=15, n_estimators=1000;, score=nan total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "#pickling the model \n",
    "from joblib import dump\n",
    "\n",
    "dump(reg_pipe, \"models/lin_reg.joblib\") \n",
    "dump(gbr_pipe, \"models/gbr_init.joblib\")\n",
    "dump(gbr_improved_pipe, \"models/gbr_improved.joblib\")\n",
    "dump(gbr_improved_pipe_2, \"models/gbr_improved_2.joblib\")\n",
    "dump(gbr_improved_pipe_3, \"models/gbr_improved_3.joblib\")\n",
    "dump(randfor_pipe, \"models/rfr.joblib\")\n",
    "dump(dectree_pipe, \"models/dec_tree.joblib\")\n",
    "dump(xtratree_pipe, \"models/xtra_tree.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
